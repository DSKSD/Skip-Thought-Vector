{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1506.06726.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab;tagger=Mecab()\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.0+751198f'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w] if w in to_ix.keys() else to_ix[\"<UNK>\"], seq))\n",
    "    tensor = Variable(torch.LongTensor(idxs)).cuda() if USE_CUDA else Variable(torch.LongTensor(idxs))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비 & 전처리 (문장 단위로 나누기) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"insight_life_sent.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "percentage = re.compile(\"\\d+[.]\\d+%\")\n",
    "email = re.compile(\"[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*\")\n",
    "url = re.compile(\"((http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)|www.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\")\n",
    "com = re.compile(\"[.]{2,}\")\n",
    "numbering = re.compile(\"\\d+[.]\")\n",
    "news = re.compile(\"[가-힣]+(뉴스|일보)\")\n",
    "reporter = re.compile(\"[가-힣]{3} 기자 = \")\n",
    "photo = re.compile(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [d[1:-1] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = [[d for d in dd if d!=\"연합뉴스\"] for dd in data]\n",
    "data = [[d for d in dd if \"SBS\" not in d] for dd in data]\n",
    "data = [[d for d in dd if \"KBS\" not in d] for dd in data]\n",
    "data = [[d for d in dd if \"MBC\" not in d] for dd in data]\n",
    "data = [[d.replace(\"\\'\",\"\") for d in dd] for dd in data]\n",
    "data = [[d for d in dd if \"Facebook\" not in d] for dd in data]\n",
    "data = [[d for d in dd if \"Youtube\" not in d] for dd in data]\n",
    "data = [[d for d in dd if \"Instagram\" not in d] for dd in data]\n",
    "data = [[d for d in dd if d!=\"Littlethings\"] for dd in data]\n",
    "data = [[d for d in dd if \"imagesbank\" not in d.lower()] for dd in data]\n",
    "data = [[d for d in dd if \"기사와 관련 없는 자료 사진\" not in d] for dd in data]\n",
    "data = [[news.sub(\"\",d) for d in dd] for dd in data]\n",
    "data = [[email.sub(\"\",d) for d in dd] for dd in data]\n",
    "data = [[url.sub(\"\",d) for d in dd] for dd in data]\n",
    "data = [[d.replace(\"[인사이트]\",\"\") for d in dd] for dd in data]\n",
    "data = [[reporter.sub(\"\",d).strip() for d in dd] for dd in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "for d in data:\n",
    "    tris = list(nltk.trigrams(d))\n",
    "    for tri in tris:\n",
    "        X.append(tri[1].strip())\n",
    "        y.append([tri[0].strip(),tri[2].strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 길이 분포  파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_X = [tagger.morphs(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LENGTH=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프로세싱(패딩) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_X=[]\n",
    "p_y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for x in X:\n",
    "    temp = tagger.morphs(x) \n",
    "    if len(temp)<LENGTH:\n",
    "        temp.append('<EOS>')\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "\n",
    "    p_X.append(temp)\n",
    "    \n",
    "    \n",
    "for yy in y:\n",
    "    temp_y=[]\n",
    "    temp = tagger.morphs(yy[0])\n",
    "    if len(temp)<LENGTH:\n",
    "        temp.append('<EOS>')\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "    temp_y.append(temp)\n",
    "    \n",
    "    temp = tagger.morphs(yy[1])\n",
    "    if len(temp)<LENGTH:\n",
    "        temp.append('<EOS>')\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "    temp_y.append(temp)\n",
    "    p_y.append(temp_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab index dic 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y1,y2 = zip(*p_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_vocab = flatten(p_X) + flatten(y1) + flatten(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word2index = {'<PAD>': 0, '<UNK>':1,'<SOS>':2,'<EOS>':3}\n",
    "for token in all_vocab:\n",
    "    if token not in word2index.keys():\n",
    "        word2index[token]=len(word2index)\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13069"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(p_X,p_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs=[]\n",
    "\n",
    "for tr in train_data:\n",
    "    \n",
    "    temp = prepare_sequence(tr[0],word2index)\n",
    "    temp = temp.view(1,-1)\n",
    "    \n",
    "    temp2 = prepare_sequence(tr[1][0],word2index)\n",
    "    temp2 = temp2.view(1,-1)\n",
    "    temp3 = prepare_sequence(tr[1][1],word2index)\n",
    "    temp3 = temp3.view(1,-1)\n",
    "    \n",
    "    inputs.append((temp,temp2,temp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15299"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 선언 (Bi-Skip 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BiSkipEncoder(nn.Module):\n",
    "    def __init__(self, input_size,embedding_size, hidden_size,n_layers=1):\n",
    "        super(BiSkipEncoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True,bidirectional=True)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        #self.lstm.weight.data.\n",
    "    \n",
    "    def init_hidden(self,input):\n",
    "        hidden = Variable(torch.randn(self.n_layers*2, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.randn(self.n_layers*2, input.size(0), self.hidden_size))\n",
    "        context = Variable(torch.randn(self.n_layers*2, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.randn(self.n_layers*2, input.size(0), self.hidden_size))\n",
    "        return (hidden,context)\n",
    "    \n",
    "    def make_translation_matrix(self,global_embedding_size):\n",
    "        self.global_embedding_size = global_embedding_size\n",
    "        self.translation_weight = nn.Linear(self.global_embedding_size,self.embedding_size)\n",
    "        self.translation_weight.bias.data.fill_(0)\n",
    "        self.translation_weight.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        \n",
    "    def translation_matrix(self,word_vecs):\n",
    "        \"\"\"\n",
    "        word_vecs : BxD (global_embedding_size) FloatTensor\n",
    "        \"\"\"\n",
    "        v_prime = self.translation_weight(word_vecs)\n",
    "        \n",
    "        return v_prime # BxD (embedding_size)\n",
    "\n",
    "    \n",
    "    def forward(self, input,input_masking):\n",
    "        \"\"\"\n",
    "        input : B,T (LongTensor)\n",
    "        input_masking : B,T (PAD 마스킹한 ByteTensor)\n",
    "        \n",
    "        <PAD> 제외한 리얼 Context를 다시 만들어서 아웃풋으로\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hidden = self.init_hidden(input)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        output, self.hidden = self.lstm(embedded, self.hidden)\n",
    "        \n",
    "        real_context=[]\n",
    "        \n",
    "        for i,o in enumerate(output): # B,T,D\n",
    "            real_length = input_masking[i].data.tolist().count(0) # 실제 길이\n",
    "            real_context.append(o[real_length-1])\n",
    "            \n",
    "        return output, torch.cat(real_context).view(input.size(0),-1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BiSkipDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,output_size,embedding_size,hidden_size,max_len=60,n_layers=1,dropout_p=0.1):\n",
    "        super(BiSkipDecoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_len=max_len\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "\n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size) #TODO encoder와 공유하도록 하고 학습되지 않게..\n",
    "\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.embedding_size+self.hidden_size, self.hidden_size, self.n_layers, batch_first=True)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.out.bias.data.fill_(0)\n",
    "        self.out.weight.data.uniform_(-0.1, 0.1)\n",
    "        #self.lstm.weight.data.\n",
    "    \n",
    "    def init_hidden(self,input):\n",
    "        hidden = Variable(torch.randn(self.n_layers*1, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.randn(self.n_layers*2, input.size(0), self.hidden_size))\n",
    "        context = Variable(torch.randn(self.n_layers*1, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.randn(self.n_layers*2, input.size(0), self.hidden_size))\n",
    "        return (hidden,context)\n",
    "    \n",
    "    def forward(self, input,enc_context,training=True):\n",
    "        \"\"\"\n",
    "        input : B,L(length)\n",
    "        enc_context : B,1,D\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(input)\n",
    "        hidden = self.init_hidden(input)\n",
    "        #embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode=[]\n",
    "        for i in range(self.max_len):\n",
    "        \n",
    "            _, hidden = self.lstm(torch.cat((embedded,enc_context),2), hidden)\n",
    "            #concated = torch.cat((hidden[0],enc_context.transpose(0,1)),2)\n",
    "            score = self.out(hidden[0].squeeze(0))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            _,input = torch.max(softmaxed,1)\n",
    "            embedded = self.embedding(input.unsqueeze(1))\n",
    "\n",
    "        # Decode hidden states of all time step\n",
    "        scores = torch.cat(decode,1)\n",
    "        del decode\n",
    "        \n",
    "        return scores.view(input.size(0)*self.max_len,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE=0.001\n",
    "EMBEDDING_SIZE=300\n",
    "HIDDEN_SIZE=600\n",
    "BATCH_SIZE=16\n",
    "LENGTH=60\n",
    "STEP_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder = BiSkipEncoder(len(word2index),EMBEDDING_SIZE,HIDDEN_SIZE)\n",
    "decoder = BiSkipDecoder(len(word2index),EMBEDDING_SIZE,HIDDEN_SIZE*2,LENGTH)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "encoder.init_weights()\n",
    "decoder.init_weights()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optim= optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "dec_optim = optim.Adam(decoder.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0  epoch 0  :  18.968\n",
      "Step 0  epoch 10  :  15.6805\n",
      "Step 0  epoch 20  :  12.9234\n",
      "Step 0  epoch 30  :  12.6665\n",
      "Step 0  epoch 40  :  12.6242\n",
      "Step 0  epoch 50  :  12.5704\n",
      "Step 0  epoch 60  :  12.4588\n",
      "Step 0  epoch 70  :  12.2012\n",
      "Step 0  epoch 80  :  12.0843\n",
      "Step 0  epoch 90  :  11.9483\n",
      "Step 0  epoch 100  :  12.0611\n",
      "Step 0  epoch 110  :  11.8709\n",
      "Step 0  epoch 120  :  11.6767\n",
      "Step 0  epoch 130  :  11.2557\n",
      "Step 0  epoch 140  :  11.5428\n",
      "Step 0  epoch 150  :  11.1637\n",
      "Step 0  epoch 160  :  11.3054\n",
      "Step 0  epoch 170  :  10.7758\n",
      "Step 0  epoch 180  :  10.9701\n",
      "Step 0  epoch 190  :  11.1216\n",
      "Step 0  epoch 200  :  11.5359\n",
      "Step 0  epoch 210  :  11.1288\n",
      "Step 0  epoch 220  :  10.7807\n",
      "Step 0  epoch 230  :  10.2768\n",
      "Step 0  epoch 240  :  10.4039\n",
      "Step 0  epoch 250  :  10.2145\n",
      "Step 0  epoch 260  :  10.1327\n",
      "Step 0  epoch 270  :  9.57228\n",
      "Step 0  epoch 280  :  10.0183\n",
      "Step 0  epoch 290  :  10.0192\n",
      "Step 0  epoch 300  :  9.66686\n",
      "Step 0  epoch 310  :  9.35285\n",
      "Step 0  epoch 320  :  9.18062\n",
      "Step 0  epoch 330  :  8.79438\n",
      "Step 0  epoch 340  :  9.36516\n",
      "Step 0  epoch 350  :  8.76895\n",
      "Step 0  epoch 360  :  9.11617\n",
      "Step 0  epoch 370  :  9.10518\n",
      "Step 0  epoch 380  :  8.62395\n",
      "Step 0  epoch 390  :  9.52931\n",
      "Step 0  epoch 400  :  9.39134\n",
      "Step 0  epoch 410  :  9.49943\n",
      "Step 0  epoch 420  :  8.95994\n",
      "Step 0  epoch 430  :  8.81787\n",
      "Step 0  epoch 440  :  8.54463\n",
      "Step 0  epoch 450  :  8.4776\n",
      "Step 0  epoch 460  :  8.69266\n",
      "Step 0  epoch 470  :  9.08633\n",
      "Step 0  epoch 480  :  8.67784\n",
      "Step 0  epoch 490  :  8.82159\n",
      "Step 0  epoch 500  :  8.58776\n",
      "Step 0  epoch 510  :  8.19915\n",
      "Step 0  epoch 520  :  7.71391\n",
      "Step 0  epoch 530  :  8.43473\n",
      "Step 0  epoch 540  :  8.10141\n",
      "Step 0  epoch 550  :  7.20186\n",
      "Step 0  epoch 560  :  7.88341\n",
      "Step 0  epoch 570  :  7.50112\n",
      "Step 0  epoch 580  :  8.23595\n",
      "Step 0  epoch 590  :  7.46791\n",
      "Step 0  epoch 600  :  7.62474\n",
      "Step 0  epoch 610  :  7.25173\n",
      "Step 0  epoch 620  :  6.97194\n",
      "Step 0  epoch 630  :  7.83395\n",
      "Step 0  epoch 640  :  6.79332\n",
      "Step 0  epoch 650  :  7.27846\n",
      "Step 0  epoch 660  :  7.7844\n",
      "Step 0  epoch 670  :  7.48582\n",
      "Step 0  epoch 680  :  6.67726\n",
      "Step 0  epoch 690  :  6.09756\n",
      "Step 0  epoch 700  :  7.30267\n",
      "Step 0  epoch 710  :  7.30264\n",
      "Step 0  epoch 720  :  6.62229\n",
      "Step 0  epoch 730  :  6.53393\n",
      "Step 0  epoch 740  :  7.03943\n",
      "Step 0  epoch 750  :  6.10275\n",
      "Step 0  epoch 760  :  7.40968\n",
      "Step 0  epoch 770  :  6.95198\n",
      "Step 0  epoch 780  :  6.526\n",
      "Step 0  epoch 790  :  6.92388\n",
      "Step 0  epoch 800  :  6.36181\n",
      "Step 0  epoch 810  :  6.58017\n",
      "Step 0  epoch 820  :  6.17736\n",
      "Step 0  epoch 830  :  7.27817\n",
      "Step 0  epoch 840  :  6.60551\n",
      "Step 0  epoch 850  :  6.10858\n",
      "Step 0  epoch 860  :  6.96083\n",
      "Step 0  epoch 870  :  6.42738\n",
      "Step 0  epoch 880  :  6.55147\n",
      "Step 0  epoch 890  :  6.2231\n",
      "Step 0  epoch 900  :  6.23603\n",
      "Step 0  epoch 910  :  6.17967\n",
      "Step 0  epoch 920  :  5.60733\n",
      "Step 0  epoch 930  :  6.42702\n",
      "Step 0  epoch 940  :  6.76046\n",
      "Step 0  epoch 950  :  6.37868\n"
     ]
    }
   ],
   "source": [
    "for step in range(STEP_SIZE):\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE,inputs)):\n",
    "        x,y_1,y_2 = zip(*batch)\n",
    "        x = torch.cat(x)\n",
    "        y_1 = torch.cat(y_1)\n",
    "        y_2 = torch.cat(y_2)\n",
    "\n",
    "        x_mask = torch.cat([Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, t.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, t.data)))) for t in x]).view(BATCH_SIZE,-1)\n",
    "    \n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "\n",
    "        output, hidden_c = encoder(x,x_mask)\n",
    "        y_1_input = Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).transpose(1,0)\n",
    "        y_2_input = Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).transpose(1,0)\n",
    "\n",
    "        y_1_score = decoder(y_1_input,hidden_c)\n",
    "        y_2_score = decoder(y_2_input,hidden_c)\n",
    "\n",
    "        loss_1 = loss_function(y_1_score,y_1.view(-1))\n",
    "        loss_2 = loss_function(y_2_score,y_2.view(-1))\n",
    "\n",
    "        loss = loss_1+loss_2\n",
    "        losses.append(loss.data.cpu().numpy()[0] if USE_CUDA else loss.data.numpy()[0])\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 10.0)\n",
    "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 10.0)\n",
    "\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "\n",
    "        if i % 100==0:\n",
    "            print(\"Step\",step,\" epoch\",i,\" : \",np.mean(losses))\n",
    "            losses=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LSTM cell 직접 customize\n",
    "2. LSTM onthogonal init\n",
    "3. CrossEntropy(ignore_index=0) 적용 # 이건 버전업하면 될듯\n",
    "4. Vocaburary Expansion with GloVe or Word2Vec\n",
    "5. 실제 결과 검증 (How to?! => 영어 데이터로)\n",
    "6. Gutenburg corpus로 학습시키고 SentEval해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
